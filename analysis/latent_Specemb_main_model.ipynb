{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcedce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1:\n",
    "# Generate data after each epoch of training, if less than\n",
    "# 10% error rate, and is a legit SMILES\n",
    "# append to the real data\n",
    "# Otherwise, append to fake data\n",
    "\n",
    "# ADDING REINFORCEMENT MECHANISM\n",
    "# Regenerate Normal sampling (define ranges), default: uniform\n",
    "\n",
    "# IMPORTANT!!!!!!!!!!!!! DO NOT DROP DUPLICATE FOR RESULT .CSV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from progressbar import ProgressBar\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import Crippen as logp\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n",
    "                          Concatenate, ReLU, Flatten, Dense, Embedding,\n",
    "                          BatchNormalization, Activation, SpatialDropout1D,\n",
    "                          Conv2D, MaxPooling2D, UpSampling2D, Lambda)\n",
    "from tensorflow.keras.models     import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses     import mse, binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.metrics import  mean_squared_error as mse_keras\n",
    "from tensorflow.keras.backend import argmax as argmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "from tensorflow import random as randomtf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "from   matplotlib.lines import Line2D\n",
    "from   matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as tk\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import seaborn as sns\n",
    "\n",
    "from chainer_chemistry.dataset.preprocessors import GGNNPreprocessor, construct_atomic_number_array\n",
    "preprocessor = GGNNPreprocessor()\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "from rdkit import Chem\n",
    "\n",
    "import ntpath\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\"\"\" fix all the seeds,results are still slighthly different \"\"\"\n",
    "randomtf.set_seed(10)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(420)\n",
    "random.seed(123450)\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3667)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)\n",
    "#tf.set_random_seed(1234)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b638de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1:\n",
    "# Generate data after each epoch of training, if less than\n",
    "# 10% error rate, and is a legit SMILES\n",
    "# append to the real data\n",
    "# Otherwise, append to fake data\n",
    "\n",
    "# ADDING REINFORCEMENT MECHANISM\n",
    "# Regenerate Normal sampling (define ranges), default: uniform\n",
    "\n",
    "# IMPORTANT!!!!!!!!!!!!! DO NOT DROP DUPLICATE FOR RESULT .CSV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from progressbar import ProgressBar\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import Crippen as logp\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,\n",
    "                          Concatenate, ReLU, Flatten, Dense, Embedding,\n",
    "                          BatchNormalization, Activation, SpatialDropout1D,\n",
    "                          Conv2D, MaxPooling2D, UpSampling2D, Lambda)\n",
    "from tensorflow.keras.models     import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses     import mse, binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.metrics import  mean_squared_error as mse_keras\n",
    "from tensorflow.keras.backend import argmax as argmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "from tensorflow import random as randomtf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "from   matplotlib.lines import Line2D\n",
    "from   matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as tk\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import seaborn as sns\n",
    "\n",
    "from chainer_chemistry.dataset.preprocessors import GGNNPreprocessor, construct_atomic_number_array\n",
    "preprocessor = GGNNPreprocessor()\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "from rdkit import Chem\n",
    "\n",
    "import ntpath\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\"\"\" fix all the seeds,results are still slighthly different \"\"\"\n",
    "randomtf.set_seed(10)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(420)\n",
    "random.seed(123450)\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3667)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)\n",
    "#tf.set_random_seed(1234)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\"\"\" reading and preprocessing data\"\"\"\n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/image_train.pickle', 'rb') as f:\n",
    "    X_smiles_train, SMILES_train, X_atoms_train, X_bonds_train, y_train0 = pickle.load(f)\n",
    "\n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/image_test.pickle', 'rb') as f:\n",
    "    X_smiles_val, SMILES_val, X_atoms_val, X_bonds_val, y_val0 = pickle.load(f)\n",
    "\n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "tokenizer[0] = ' '\n",
    "\n",
    "with open('./../data/trainingsets/60000_train_regular_qm9/train_GAN.pickle', 'rb') as f:\n",
    "    X_smiles_gantrain, SMILES_gantrain, cv_gantrain = pickle.load(f)\n",
    "\n",
    "\n",
    "X_smiles_gantrain_ = []\n",
    "for i in X_smiles_gantrain:\n",
    "  X_smiles_gantrain_.append(i)\n",
    "X_smiles_gantrain = np.array (X_smiles_gantrain_)\n",
    "print (X_smiles_gantrain.shape)\n",
    "\n",
    "cv_gantrain_ = []\n",
    "for i in cv_gantrain:\n",
    "  cv_gantrain_.append(i)\n",
    "cv_gantrain = np.array (cv_gantrain_)\n",
    "print (cv_gantrain.shape)\n",
    "\n",
    "SMILES_gantrain_ = []\n",
    "for smiles in SMILES_gantrain:\n",
    "  SMILES_gantrain_.append(smiles)\n",
    "SMILES_gantrain = np.array (SMILES_gantrain_)\n",
    "print (SMILES_gantrain.shape)\n",
    "\n",
    "# Subsampling has been done in the data preprocesses\n",
    "print ('X_smiles_train shape: ', X_smiles_train.shape)\n",
    "print ('X_smiles_test shape: ', X_smiles_val.shape)\n",
    "#print ('last SMILES train: ', SMILES_train[-1])\n",
    "\n",
    "## Outlier removal 1.5*IQR rule\n",
    "# Train samples\n",
    "IQR = - np.quantile(y_train0, 0.25) + np.quantile(y_train0, 0.75)\n",
    "lower_bound, upper_bound = np.quantile(y_train0, 0.25) - 1.5 * IQR, np.quantile(y_train0, 0.75) + 1.5 * IQR\n",
    "idx = np.where((y_train0 >= lower_bound) & (y_train0 <= upper_bound))\n",
    "\n",
    "y_train = y_train0[idx]\n",
    "X_smiles_train = X_smiles_train[idx]\n",
    "X_atoms_train = X_atoms_train[idx]\n",
    "X_bonds_train = X_bonds_train[idx]\n",
    "\n",
    "# Test samples\n",
    "IQR = - np.quantile(y_val0, 0.25) + np.quantile(y_val0, 0.75)\n",
    "lower_bound, upper_bound = np.quantile(y_val0, 0.25) - 1.5 * IQR, np.quantile(y_val0, 0.75) + 1.5 * IQR\n",
    "idx = np.where((y_val0 >= lower_bound) & (y_val0 <= upper_bound))\n",
    "\n",
    "y_val = y_val0[idx]\n",
    "X_smiles_val = X_smiles_val[idx]\n",
    "X_atoms_val = X_atoms_val[idx]\n",
    "X_bonds_val = X_bonds_val[idx]\n",
    "\n",
    "\n",
    "# subsampling\n",
    "idx = np.random.choice(len(y_train), int(len(y_train) * 0.016), replace = False)\n",
    "y_train = y_train[idx]\n",
    "X_smiles_train = X_smiles_train[idx]\n",
    "X_atoms_train = X_atoms_train[idx]\n",
    "X_bonds_train = X_bonds_train[idx]\n",
    "\n",
    "idx = np.random.choice(len(cv_gantrain), int(len(cv_gantrain) * 0.016), replace=False)\n",
    "cv_gantrain = cv_gantrain[idx]\n",
    "X_smiles_gantrain = X_smiles_gantrain[idx]\n",
    "SMILES_gantrain = SMILES_gantrain[idx]\n",
    "\n",
    "# normalize the bond and aotm matrices:\n",
    "def norm(X: ndarray) -> ndarray:\n",
    "    X = np.where(X == 0, -1.0, 1.0)\n",
    "    return X\n",
    "\n",
    "X_atoms_train, X_bonds_train = (norm(X_atoms_train),\n",
    "                                norm(X_bonds_train))\n",
    "X_atoms_val, X_bonds_val = (norm(X_atoms_val),\n",
    "                            norm(X_bonds_val))\n",
    "# normalize the property\n",
    "s_min1 = np.min (y_train)\n",
    "s_max1 = np.max (y_train)\n",
    "\n",
    "s_min2 = np.min(y_val)\n",
    "s_max2 = np.max(y_val)\n",
    "\n",
    "s_min = min(s_min1, s_min2)\n",
    "s_max = max(s_max1, s_max2)\n",
    "\n",
    "s_min_dataset, s_max_dataset = s_min, s_max\n",
    "s_min_norm, s_max_norm = s_min_dataset, s_max_dataset\n",
    "\n",
    "y_val   = (y_val -   s_min_norm) / (s_max_norm - s_min_norm)\n",
    "y_train = (y_train - s_min_norm) / (s_max_norm - s_min_norm)\n",
    "cv_gantrain = (cv_gantrain - s_min_norm) / (s_max_norm - s_min_norm)\n",
    "print (\"min and max dataset and val normalized\", s_min, s_max, np.min(y_val), np.max(y_val))\n",
    "print (\"min and max dataset and train normalized\", s_min, s_max, np.min(y_train), np.max(y_train))\n",
    "print (\"min and max used for normalization: \", s_min_norm, s_max_norm)\n",
    "\n",
    "encoder = load_model('./../data/nns_9HA_noemb_6b6/keep/encoder_newencinp.h5')\n",
    "decoder = load_model('./../data/nns_9HA_noemb_6b6/keep/decoder_newencinp.h5')\n",
    "regressor =     load_model('./../data/nns_9HA_noemb_6b6/keep/regressor.h5')\n",
    "regressor_top = load_model('./../data/nns_9HA_noemb_6b6/keep/regressor_top.h5')\n",
    "generator = load_model    ('./../data/nns_9HA_noemb_6b6/keep/generator_new.h5')\n",
    "discriminator= load_model ('./../data/nns_9HA_noemb_6b6/keep/discriminator_new.h5')\n",
    "\n",
    "# No need following block if gen. samples are OK\n",
    "\"\"\"\n",
    "N = 30\n",
    "n_sample = 700\n",
    "\n",
    "gen_error = []\n",
    "gen_smiles = []\n",
    "sample_ys = []\n",
    "preds = []\n",
    "gen_atoms_embedding = []\n",
    "gen_bonds_embedding = []\n",
    "\n",
    "regressor_top.trainable = False\n",
    "regressor.trainable = False\n",
    "generator.trainable = False\n",
    "discriminator.trainable = False\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "pbar = ProgressBar()\n",
    "for hc in pbar(range(n_sample)):\n",
    "    try:\n",
    "        # get it back to original of s_min to s_max\n",
    "        sample_y = np.random.uniform(s_min_norm, s_max_norm, size=[1,])\n",
    "        print (sample_y)\n",
    "        sample_y = np.round(sample_y, 4)\n",
    "        sample_y = sample_y * np.ones([N,])\n",
    "        sample_y_ = (sample_y - s_min_norm) / (s_max_norm - s_min_norm)\n",
    "        sample_z = np.random.normal(0, 1, size = (N, 128))\n",
    "\n",
    "        regressor_top.trainable = False\n",
    "        regressor.trainable = False\n",
    "\n",
    "        sample_atoms_embedding, sample_bonds_embedding = generator.predict([sample_z, sample_y_])\n",
    "        dec_embedding = np.concatenate([sample_atoms_embedding, sample_bonds_embedding], axis = -1)\n",
    "\n",
    "        softmax_smiles = decoder.predict(dec_embedding)[0]\n",
    "        argmax_smiles = np.argmax(softmax_smiles, axis = 2)\n",
    "        print ('shape argmax_smiles', argmax_smiles.shape)\n",
    "        smiles = to_categorical(argmax_smiles, num_classes=23)\n",
    "        SHAPE = list(smiles.shape) + [1]\n",
    "        print ('shape line 767', SHAPE)\n",
    "        smiles = smiles.reshape(SHAPE)\n",
    "\n",
    "        latent_encoder_atom, latent_encoder_bond, _ = encoder.predict([smiles])\n",
    "        pred = regressor.predict([latent_encoder_atom, latent_encoder_bond]).reshape([-1])\n",
    "        pred = pred * (s_max_norm - s_min_norm) + s_min_norm\n",
    "\n",
    "        gen_errors = np.abs((pred - sample_y) / sample_y).reshape([-1])\n",
    "\n",
    "        #accurate = np.where(gen_errors <= 0.2)[0]\n",
    "        #gen_errors = gen_errors[accurate]\n",
    "        #pred = pred[accurate]\n",
    "\n",
    "        #sample_y = sample_y[accurate]\n",
    "        #sample_atoms_embedding = sample_atoms_embedding[accurate]\n",
    "        #sample_bonds_embedding = sample_bonds_embedding[accurate]\n",
    "\n",
    "        smiles = decoder.predict(dec_embedding)[0]\n",
    "        smiles = np.argmax(smiles, axis = 2).reshape(smiles.shape[0], 35)\n",
    "\n",
    "        generated_smiles = []\n",
    "        for S in smiles:\n",
    "            c_smiles = ''\n",
    "            for s in S:\n",
    "                c_smiles += tokenizer[s]\n",
    "            c_smiles = c_smiles.rstrip()\n",
    "            generated_smiles.append(c_smiles)\n",
    "        generated_smiles = np.array(generated_smiles)\n",
    "        #generated_smiles = generated_smiles [accurate]\n",
    "        all_gen_smiles = []\n",
    "        idx = []\n",
    "        for i, smiles in enumerate(generated_smiles):\n",
    "            all_gen_smiles.append(smiles[:-1])\n",
    "\n",
    "            if ' ' in smiles[:-1]:\n",
    "                continue\n",
    "            #m = Chem.MolFromSmiles(smiles[:-1], sanitize=False)\n",
    "            m = Chem.MolFromSmiles(smiles[:-1], sanitize=True)\n",
    "            if m is not None:\n",
    "                if len(construct_atomic_number_array(m)) <= 9:\n",
    "                    idx.append(i)\n",
    "\n",
    "        idx = np.array(idx)\n",
    "        all_gen_smiles = np.array(all_gen_smiles)\n",
    "        print ('all gen smiels shape', all_gen_smiles.shape)\n",
    "        print ('gen_errors shape', gen_errors.shape)\n",
    "        gen_smiles.extend(list(all_gen_smiles[idx]))\n",
    "        gen_error.extend(list(gen_errors[idx]))\n",
    "        sample_ys.extend(list(sample_y[idx]))\n",
    "        gen_atoms_embedding.extend(sample_atoms_embedding[idx])\n",
    "        gen_bonds_embedding.extend(sample_bonds_embedding[idx])\n",
    "        \n",
    "\n",
    "        preds.extend(list(pred[idx]))\n",
    "    except:\n",
    "        print('Did not discover SMILES for HC: {}'.format(sample_y))\n",
    "        pass\n",
    " \n",
    "output = {}\n",
    "\n",
    "for i, s in enumerate (gen_smiles):\n",
    "\n",
    "    ss = Chem.MolToSmiles(Chem.MolFromSmiles(s))\n",
    "    gen_smiles[i] = ss\n",
    "\n",
    "output['SMILES'] = gen_smiles\n",
    "output['des_cv'] = sample_ys\n",
    "output['pred_cv'] = preds\n",
    "output['Err_pred_des'] = gen_error\n",
    "\n",
    "with open('./../experiments/regular_9HA_6b6latent/latent/gen_atoms_bonds.pickle', 'wb') as f:\n",
    "    pickle.dump((gen_atoms_embedding, gen_bonds_embedding), f)\n",
    "\n",
    "output = pd.DataFrame(output)\n",
    "output.reset_index(drop = True, inplace = True)\n",
    "output.to_csv ('./../experiments/regular_9HA_6b6latent/latent/Regular_noscreenrelug.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "# read the generated data\n",
    "csv_name = './../experiments/regular_9HA_6b6latent/latent/Regular_noscreenrelug.csv'\n",
    "gen_SMILES = pd.read_csv(csv_name)\n",
    "gen_smiles = gen_SMILES ['SMILES']\n",
    "sample_ys = gen_SMILES ['des_cv']\n",
    "preds = gen_SMILES ['pred_cv']\n",
    "gen_error = gen_SMILES ['Err_pred_des']\n",
    "\n",
    "with open('./../experiments/regular_9HA_6b6latent/latent/gen_atoms_bonds.pickle', 'rb') as f:\n",
    "    gen_atoms_embedding, gen_bonds_embedding = pickle.load(f)\n",
    "\n",
    "print ('preds', preds)\n",
    "print ('des cv', sample_ys)\n",
    "\n",
    "plt.close()\n",
    "plt.hist(gen_error)\n",
    "plt.savefig(\"gen_error_hist.png\")\n",
    "\n",
    "# total # of samples\n",
    "N = len(gen_error)\n",
    "# Explained Variance R2 from sklearn.metrics.explained_variance_score\n",
    "explained_variance_R2_DFT_des = explained_variance_score(sample_ys, preds)\n",
    "print (\"explained_varice_R2_DFT_des\", explained_variance_R2_DFT_des)\n",
    "rsquared = r2_score (sample_ys, preds)\n",
    "print (rsquared)\n",
    "\n",
    "gen_atoms_embedding = np.array(gen_atoms_embedding)\n",
    "gen_bonds_embedding = np.array(gen_bonds_embedding)\n",
    "\n",
    "# create classes for heat capacity\n",
    "# using cv_gantrain, uniformly distributed.\n",
    "y_train = cv_gantrain * (s_max_norm - s_min_norm) + s_min_norm\n",
    "plt.clf()\n",
    "plt.hist(y_train)\n",
    "plt.savefig('cv_gantrainhist.png', dpi=100)\n",
    "\n",
    "plt.clf()\n",
    "plt.hist(preds, color='blue', alpha=1)\n",
    "plt.hist(sample_ys, color='red', alpha=1)\n",
    "plt.savefig('pred_des.png', dpi=100)\n",
    "\n",
    "y_class = y_train\n",
    "print (y_class) \n",
    "\"\"\"\n",
    "# 10 classes\n",
    "y_class = np.where(y_train <= Qs[0], 0, y_class)\n",
    "y_class = np.where((y_train > Qs[0]) & (y_train <= Qs[1]), 1, y_class)\n",
    "y_class = np.where((y_train > Qs[1]) & (y_train <= Qs[2]), 2, y_class)\n",
    "y_class = np.where((y_train > Qs[2]) & (y_train <= Qs[3]), 3, y_class)\n",
    "y_class = np.where((y_train > Qs[3]) & (y_train <= Qs[4]), 4, y_class)\n",
    "y_class = np.where((y_train > Qs[4]) & (y_train <= Qs[5]), 5, y_class)\n",
    "y_class = np.where((y_train > Qs[5]) & (y_train <= Qs[6]), 6, y_class)\n",
    "y_class = np.where((y_train > Qs[6]) & (y_train <= Qs[7]), 7, y_class)\n",
    "y_class = np.where((y_train > Qs[7]) & (y_train <= Qs[8]), 8, y_class)\n",
    "y_class = np.where(y_train > Qs[8], 9, y_class)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# 5 classes\n",
    "y_class = np.where(y_train <= Qs[1], 0, y_class)\n",
    "y_class = np.where((y_train > Qs[1]) & (y_train <= Qs[3]), 1, y_class)\n",
    "y_class = np.where((y_train > Qs[3]) & (y_train <= Qs[5]), 2, y_class)\n",
    "y_class = np.where((y_train > Qs[5]) & (y_train <= Qs[7]), 3, y_class)\n",
    "y_class = np.where(y_train > Qs[7], 4, y_class)\n",
    "\"\"\"\n",
    "# use the same classes\n",
    "Qs_gen = np.quantile(preds, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "Qs = np.quantile(y_train, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "y_class_val = preds\n",
    "y_class = y_train\n",
    "print (\"quantile of train samples: \", Qs)\n",
    "\n",
    "# 3 classes\n",
    "y_class_val = np.where(preds <= (Qs_gen[1]+Qs_gen[2])/2, 0, y_class_val)\n",
    "y_class_val = np.where((preds > (Qs_gen[1]+Qs_gen[2])/2) & (preds <= Qs_gen[4]), 1, y_class_val)\n",
    "y_class_val = np.where((preds> Qs_gen[4]) & (preds <= (Qs_gen[6]+Qs_gen[7])/2), 2, y_class_val)\n",
    "y_class_val = np.where(preds > (Qs_gen[6]+Qs_gen[7])/2, 3, y_class_val)\n",
    "\n",
    "# 3 classes\n",
    "y_class = np.where(y_train <= (Qs[1]+Qs[2])/2, 0, y_class)\n",
    "y_class = np.where((y_train > (Qs[1]+Qs[2])/2) & (y_train <= Qs[4]), 1, y_class)\n",
    "y_class = np.where((y_train > Qs[4]) & (y_train <= (Qs[6]+Qs[7])/2), 2, y_class)\n",
    "y_class = np.where(y_train > (Qs[6]+Qs[7])/2, 3, y_class)\n",
    "\n",
    "print ('gen class==0', sum(y_class_val==0))\n",
    "print ('gen class==1', sum(y_class_val==1))\n",
    "print ('gen class==2', sum(y_class_val==2))\n",
    "print ('gen class==3', sum(y_class_val==3))\n",
    "\n",
    "print ('train class==0', sum(y_class==0))\n",
    "print ('train class==1', sum(y_class==1))\n",
    "print ('train class==2', sum(y_class==2))\n",
    "print ('train class==3', sum(y_class==3))\n",
    "\n",
    "# ANALYSIS\n",
    "train_atoms_embedding, train_bonds_embedding, _ = encoder.predict([X_smiles_gantrain]) \n",
    "\n",
    "X_atoms_train_ = train_atoms_embedding.reshape([train_atoms_embedding.shape[0], \n",
    "                                        6 * 6])\n",
    "X_bonds_train_ = train_bonds_embedding.reshape([train_bonds_embedding.shape[0], \n",
    "                                        6 * 6])\n",
    "\n",
    "X_atoms_test_ = gen_atoms_embedding.reshape([gen_atoms_embedding.shape[0],\n",
    "                                      6 * 6])\n",
    "X_bonds_test_ = gen_bonds_embedding.reshape([gen_bonds_embedding.shape[0], \n",
    "                                      6 * 6])\n",
    "\n",
    "\"\"\"\n",
    "### PCA ###\n",
    "pca_1 = PCA(n_components = 2)\n",
    "X_atoms_train = pca_1.fit_transform(X_atoms_train_)\n",
    "X_atoms_test = pca_1.transform(X_atoms_test_)\n",
    "\n",
    "pca_2 = PCA(n_components = 2)\n",
    "X_bonds_train = pca_2.fit_transform(X_bonds_train_)\n",
    "X_bonds_test = pca_2.transform(X_bonds_test_)\n",
    "\n",
    "# PCA1 vs. PCA2 Atoms gen and train\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(8, 6))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "plt.scatter(X_atoms_train[:,0], X_atoms_train[:,1], alpha = 0.2, c = 'blue')\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC1', fontsize=25, weight='bold')\n",
    "plt.ylabel('PC2', fontsize=25, weight='bold')\n",
    "#plt.close()\n",
    "plt.scatter(X_atoms_test[:,0], X_atoms_test[:,1], alpha = 0.2, c = 'red')\n",
    "plt.savefig(\"Mix_train_gen_atom_dist_{}Sam.png\".format(len(y_train)), bbox_inches='tight', dpi=300)\n",
    "####\n",
    "\n",
    "# PCA1 vs. PCA2 Bonds gen and train\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(8, 6))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "plt.scatter(X_bonds_train[:,0], X_bonds_train[:,1], alpha = 0.2, c = 'blue')\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC1', fontsize=25, weight='bold')\n",
    "plt.ylabel('PC2', fontsize=25, weight='bold')\n",
    "#plt.close()\n",
    "plt.scatter(X_bonds_test[:,0], X_bonds_test[:,1], alpha = 0.2, c = 'red')\n",
    "plt.savefig(\"Mix_train_gen_bonds_dist_{}Sam.png\".format(len(preds)), bbox_inches='tight', dpi=300)\n",
    "\n",
    "### concat. latent vectors ###\n",
    "X_Concat_train =  np.concatenate ([X_bonds_train_, X_atoms_train_], axis=1)\n",
    "X_Concat_test  =  np.concatenate ([X_bonds_test_, X_atoms_test_], axis=1)\n",
    "pca_2 = PCA(n_components = 2)\n",
    "X_concat_train_pca = pca_2.fit_transform(X_Concat_train)\n",
    "X_concat_test_pca = pca_2.transform(X_Concat_test)\n",
    "\n",
    "# PCA1 vs. cv gen. and train\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(10, 6))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "preds_rev = [max(preds)-i for i in preds]\n",
    "plt.scatter(X_concat_test_pca[:, 0], preds)\n",
    "#plt.legend(fontsize=12)\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC1', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('Cv', fontsize=25, fontweight='bold')\n",
    "plt.scatter(X_concat_train_pca[:, 0], y_train)\n",
    "plt.savefig(\"genvstrain_Concat_pc1vscv.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# PCA2 vs. cv gen and train\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(10, 6))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "preds_rev = [max(preds)-i for i in preds]\n",
    "plt.scatter(X_concat_test_pca[:, 1], preds, c='red')\n",
    "#plt.legend(fontsize=12)\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC2', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('Cv', fontsize=25, fontweight='bold')\n",
    "plt.scatter(X_concat_train_pca[:, 1], y_train, c='blue')\n",
    "plt.savefig(\"genvstrain_Concat_pc2vscv.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# PCA1 vs. PCA2 gen and train\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(8, 6))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "plt.scatter(X_concat_train_pca[:, 0], X_concat_train_pca[:, 1], alpha=0.2, c='blue')\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC1', fontsize=25, weight='bold')\n",
    "plt.ylabel('PC2', fontsize=25, weight='bold')\n",
    "#plt.close()\n",
    "plt.scatter(X_concat_test_pca[:, 0], X_concat_test_pca[:, 1], alpha=0.2, c='red')\n",
    "plt.savefig(\"genvstrain_concat_pc1vspc2.png\".format(len(preds)), bbox_inches='tight', dpi=300)\n",
    "\"\"\"\n",
    "\n",
    "colors = ['navy', 'mediumblue','blue', 'cornflowerblue', 'lightsteelblue', 'lavender',\n",
    "          'salmon', 'lightcoral', 'orangered', 'darkred']\n",
    "colors = ['darkblue', 'lightsteelblue', 'orangered', 'darkred']\n",
    "\"\"\"\n",
    "group_names = np.array([\"Cv<{}\".format(np.round(Qs[0])), \n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[0]), np.round(Qs[1])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[1]), np.round(Qs[2])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[2]), np.round(Qs[3])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[3]), np.round(Qs[4])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[4]), np.round(Qs[5])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[5]), np.round(Qs[6])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[6]), np.round(Qs[7])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[7]), np.round(Qs[8])),\n",
    "                            \"{}<Cv\".format(np.round(Qs[8]))])\n",
    "\n",
    "group_names_gen = np.array([\"Cv<{}\".format(np.round(Qs_gen[0])), \n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[0]), np.round(Qs_gen[1])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[1]), np.round(Qs_gen[2])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[2]), np.round(Qs_gen[3])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[3]), np.round(Qs_gen[4])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[4]), np.round(Qs_gen[5])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[5]), np.round(Qs_gen[6])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[6]), np.round(Qs_gen[7])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[7]), np.round(Qs_gen[8])),\n",
    "                            \"{}<Cv\".format(np.round(Qs_gen[8]))])\n",
    "\"\"\"\n",
    "group_names = np.array([\"Cv<{}\".format(np.round((Qs[1]+Qs[2])/2)), \n",
    "                            \"{}<Cv<{}\".format(np.round((Qs[1]+Qs[2])/2), np.round(Qs[4])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs[4]), np.round((Qs[6]+Qs[7])/2)),\n",
    "                            \"{}<Cv\".format(np.round((Qs[6]+Qs[7])/2))])\n",
    "group_names_gen = group_names\n",
    "\"\"\"\n",
    "group_names_gen = np.array([\"Cv<{}\".format(np.round(Qs_gen[2])), \n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[2]), np.round(Qs_gen[5])),\n",
    "                            \"{}<Cv<{}\".format(np.round(Qs_gen[5]), np.round(Qs_gen[7])),\n",
    "                            \"{}<Cv\".format(np.round(Qs_gen[7]))])\n",
    "\"\"\"\n",
    "\n",
    "target_ids = range(0, 4)\n",
    "# pc1 vs. pc2 concat \n",
    "X_Concat_train =  np.concatenate ([X_bonds_train_, X_atoms_train_], axis=1)\n",
    "X_Concat_test  =  np.concatenate ([X_bonds_test_, X_atoms_test_], axis=1)\n",
    "pca_2 = PCA(n_components = 3)\n",
    "X_concat_train_pca = pca_2.fit_transform(X_Concat_train)\n",
    "X_concat_test_pca = pca_2.transform(X_Concat_test)\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(10, 5))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "for i, c, label in zip(target_ids, colors, group_names):\n",
    "            plt.scatter(X_concat_test_pca[y_class_val == i, 0], \n",
    "                        X_concat_test_pca[y_class_val == i, 1], \n",
    "                        alpha=0.5, c=c, label=label)\n",
    "#plt.legend(fontsize=12)\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC1', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('PC2', fontsize=25, fontweight='bold')\n",
    "plt.savefig(\"test_conc_dist_pca.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(10, 5))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "for i, c, label in zip(target_ids, colors, group_names):\n",
    "            plt.scatter(X_concat_train_pca[y_class == i, 0],\n",
    "                        X_concat_train_pca[y_class == i, 1],\n",
    "                        alpha=0.5, c=c, label=label)\n",
    "#plt.legend(fontsize=12)\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('PC1', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('PC2', fontsize=25, fontweight='bold')\n",
    "plt.savefig(\"train_conc_dist_pca.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "perplexities = [20, 30, 40, 45, 50, 60, 100, 120]\n",
    "perplexities = [20, 30, 40, 45, 50]\n",
    "pbar = ProgressBar()\n",
    "\n",
    "# SpectralEmbedding dimension reduction\n",
    "embedding = SpectralEmbedding(n_components=2, n_jobs=-1)\n",
    "X_concat_train_SpectralEmbedding = embedding.fit_transform(X_Concat_train)\n",
    "X_concat_test_SpectralEmbedding = embedding.fit_transform(X_Concat_test)\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(10, 5))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "for i, c, label in zip(target_ids, colors, group_names):\n",
    "            plt.scatter(X_concat_test_SpectralEmbedding[y_class_val == i, 0],\n",
    "                        X_concat_test_SpectralEmbedding[y_class_val == i, 1],\n",
    "                        alpha=0.5, c=c, label=label)\n",
    "#plt.legend(fontsize=12)\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('Spec. Emb. 1', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('Spec. Emb. 2', fontsize=25, fontweight='bold')\n",
    "plt.savefig(\"test_conc_dist_SpectralEmbedding.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize =(10, 5))\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "mpl.rcParams['axes.linewidth'] = 3.5\n",
    "for i, c, label in zip(target_ids, colors, group_names):\n",
    "            plt.scatter(X_concat_train_SpectralEmbedding[y_class == i, 0],\n",
    "                        X_concat_train_SpectralEmbedding[y_class == i, 1],\n",
    "                        alpha=0.5, c=c, label=label)\n",
    "#plt.legend(fontsize=12)\n",
    "ax.tick_params(width=2, length=4)\n",
    "plt.xlabel('Spec. Emb. 1', fontsize=25, fontweight='bold')\n",
    "plt.ylabel('Spec. Emb. 2', fontsize=25, fontweight='bold')\n",
    "plt.savefig(\"train_conc_dist_SpectralEmbedding.png\", bbox_inches='tight', dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
